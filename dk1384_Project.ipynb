{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sb7mZ0PPygL5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:33.260913Z",
     "iopub.status.busy": "2021-12-20T09:01:33.250493Z",
     "iopub.status.idle": "2021-12-20T09:01:36.601870Z",
     "shell.execute_reply": "2021-12-20T09:01:36.602448Z"
    },
    "id": "sb7mZ0PPygL5",
    "outputId": "8b6ad7b1-18e2-4025-e50c-809e27fe7264"
   },
   "outputs": [],
   "source": [
    "!pip install pybullet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gym\n",
    "import pybullet_envs\n",
    "from IPython import display\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if(torch.cuda.is_available()): \n",
    "  device = torch.device('cuda:0')\n",
    "display.clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lSIPaqHCci7W",
   "metadata": {
    "id": "lSIPaqHCci7W"
   },
   "source": [
    "Custom OpenAI Gym Waler2D environment extending the PyBullet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ez2HLiQMM_b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.619330Z",
     "iopub.status.busy": "2021-12-20T09:01:36.615985Z",
     "iopub.status.idle": "2021-12-20T09:01:36.627613Z",
     "shell.execute_reply": "2021-12-20T09:01:36.628185Z"
    },
    "id": "Ez2HLiQMM_b0"
   },
   "outputs": [],
   "source": [
    "from pybullet_envs.gym_locomotion_envs import WalkerBaseBulletEnv\n",
    "from pybullet_envs.robot_locomotors import Walker2D\n",
    "\n",
    "class Walker2DCustomBulletEnv(WalkerBaseBulletEnv):\n",
    "\n",
    "  def __init__(self, render=False):\n",
    "    self.robot = Walker2D()\n",
    "    WalkerBaseBulletEnv.__init__(self, self.robot, render)\n",
    "    self.direction = 0                                               #direction forward:1 stop:0 reverse:-1\n",
    "    self.mean_abs_p = 0                                              \n",
    "    self.m=0.0\n",
    "    self.counter = 0\n",
    "    \n",
    "  def step(self,a):\n",
    "    self.counter-=1\n",
    "    if (self.counter <= 0):\n",
    "      self.direction=np.random.choice([-1,0,1],1,[0.4,0.2,0.4]).item()  #randomize direction for training\n",
    "      self.counter = np.random.randint(60,600)                          #randomize direction durations \n",
    "    return self.enjoy(a)\n",
    "    \n",
    "  def enjoy(self,a):\n",
    "    if(self.m < 100):\n",
    "      self.m+=1\n",
    "    potential_old = self.potential\n",
    "    state , reward, done, info = super().step(a)\n",
    "    potential_new = self.potential\n",
    "    progress = float(potential_new - potential_old)\n",
    "    abs_p = abs(progress)\n",
    "    self.mean_abs_p = (self.mean_abs_p*(self.m-1)+ abs_p)/self.m\n",
    "\n",
    "    state[2] *= self.direction                                      #target bearing observation\n",
    "    reward -= 0.5                                                   #reduce stayalive bonus\n",
    "    reward -= progress                                              #extract underlying velocity reward\n",
    "    reward += 1.5*max(progress*(self.direction),0)                  #correct for direction\n",
    "    reward += min(progress*(self.direction),0)                      #punish moving in wrong direction\n",
    "    reward -= 3*abs_p*(1-abs(self.direction))                       #punish moving when direction 0\n",
    "    reward += 1.5*self.mean_abs_p*(1-abs(self.direction))           #reward not moving when direction 0   \n",
    "    \n",
    "    return state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "szrHH0FNQmdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.633339Z",
     "iopub.status.busy": "2021-12-20T09:01:36.632377Z",
     "iopub.status.idle": "2021-12-20T09:01:36.635161Z",
     "shell.execute_reply": "2021-12-20T09:01:36.634445Z"
    },
    "id": "szrHH0FNQmdc"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "from gym.envs.registration import register\n",
    "  \n",
    "register(\n",
    "    id=\"Walker2DCustomBulletEnv-v0\",\n",
    "    entry_point=Walker2DCustomBulletEnv,\n",
    "    max_episode_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AhkYy_gVeyOs",
   "metadata": {
    "id": "AhkYy_gVeyOs"
   },
   "source": [
    "Network Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kp6DfLDm8ZZ-",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.671027Z",
     "iopub.status.busy": "2021-12-20T09:01:36.669958Z",
     "iopub.status.idle": "2021-12-20T09:01:36.672751Z",
     "shell.execute_reply": "2021-12-20T09:01:36.672115Z"
    },
    "id": "Kp6DfLDm8ZZ-"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "  def __init__(self,state_dim,action_dim,hidden_dim):\n",
    "    super(ActorCritic, self).__init__()\n",
    "\n",
    "    self.action_dim = action_dim\n",
    "    action_var = torch.full((action_dim,), 0.25, dtype=torch.float32)\n",
    "    self.action_var = nn.Parameter(action_var,requires_grad=True).to(device)\n",
    "\n",
    "    self.actor = nn.Sequential(\n",
    "                    nn.Linear(state_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, action_dim),\n",
    "                )\n",
    "\n",
    "    self.critic = nn.Sequential(\n",
    "                    nn.Linear(state_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, 1)\n",
    "                )\n",
    " \n",
    "  def act(self, state):\n",
    "    with torch.no_grad():\n",
    "      action_loci = self.actor(state)\n",
    "      dist = Normal(action_loci, self.action_var)\n",
    "\n",
    "      action = dist.sample()\n",
    "      logprob = dist.log_prob(action).sum(dim=-1)\n",
    "      value = self.critic(state)\n",
    "\n",
    "    return action, logprob, value\n",
    "    \n",
    "  def evaluate(self, state, action):\n",
    "    action_loci = self.actor(state)\n",
    "    dist = Normal(action_loci, self.action_var)\n",
    "        \n",
    "    logprobs = dist.log_prob(action).sum(dim=-1)\n",
    "    values = self.critic(state)\n",
    "    \n",
    "    return logprobs, values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zrNj5Hqde4IY",
   "metadata": {
    "id": "zrNj5Hqde4IY"
   },
   "source": [
    "Continuous PPO Implemented with GAE Avantages, Clipped Surogate Loss and Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aaff64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.680776Z",
     "iopub.status.busy": "2021-12-20T09:01:36.675361Z",
     "iopub.status.idle": "2021-12-20T09:01:36.701633Z",
     "shell.execute_reply": "2021-12-20T09:01:36.701032Z"
    },
    "id": "75aaff64"
   },
   "outputs": [],
   "source": [
    "class PPO():\n",
    "  def __init__(self,\n",
    "                state_dim,\n",
    "                action_dim,\n",
    "                learning_rate,\n",
    "                gamma,\n",
    "                gae_lambda,\n",
    "                n_epochs,\n",
    "                clip_rangege,\n",
    "                max_grad_norm,\n",
    "                hidden_dim,\n",
    "                vf_coef):\n",
    "    super().__init__()\n",
    "\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.gae_lambda = gae_lambda\n",
    "    self.clip_range = clip_range\n",
    "    self.n_epochs = n_epochs\n",
    "    self.vf_coef = vf_coef\n",
    "    \n",
    "    #Buffers\n",
    "    self.actions = []\n",
    "    self.states = []\n",
    "    self.logprobs = []\n",
    "    self.rewards = []\n",
    "    self.gae_advs = []\n",
    "    self.values = []\n",
    "    self.traj_start = 0\n",
    "    \n",
    "    self.policy = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
    "    self.optimizer = torch.optim.Adam([\n",
    "                                        {'params':self.policy.actor.parameters()},\n",
    "                                        {'params':self.policy.critic.parameters()},\n",
    "                                        {'params':self.policy.action_var}\n",
    "                                      ],lr =  learning_rate)\n",
    "\n",
    "    self.value_loss = nn.MSELoss()\n",
    "    \n",
    "  def clear(self):\n",
    "    self.actions.clear()\n",
    "    self.states.clear()\n",
    "    self.logprobs.clear()\n",
    "    self.rewards.clear()\n",
    "    self.gae_advs.clear()\n",
    "    self.values.clear()\n",
    "    self.traj_start=0\n",
    "      \n",
    "  def traj_update(self,padding):\n",
    "    if self.traj_start >= len(self.rewards):\n",
    "      return\n",
    "    \n",
    "    traj_rewards = self.rewards[self.traj_start:]\n",
    "    traj_values = self.values[self.traj_start:]\n",
    "    traj_values.append(padding)\n",
    "    \n",
    "    deltas = self.calculate_deltas(traj_rewards,traj_values,self.gamma)\n",
    "    \n",
    "    self.gae_advs += self.discounted_sum(deltas,self.gamma*self.gae_lambda)\n",
    "    self.rewards[self.traj_start:] = self.discounted_sum(traj_rewards,self.gamma)\n",
    "    \n",
    "    self.traj_start=len(self.rewards)      \n",
    "        \n",
    "  def discounted_sum(self,series, coef):\n",
    "    result=[]\n",
    "    accum=0\n",
    "    for term in reversed(series):\n",
    "      accum*=coef\n",
    "      accum+=term\n",
    "      result.insert(0,accum)\n",
    "    return result\n",
    "\n",
    "  def calculate_deltas(self,rew,vals,gamma):\n",
    "    gamma_v_1 = [v_i * gamma for v_i in vals[1:]]\n",
    "    return [r + g_v_1 - v for r , g_v_1, v in zip(rew, gamma_v_1, vals[:-1])]\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    state = torch.tensor(state,dtype=torch.float32).to(device)\n",
    "    self.states.append(state)\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        action, logprob, value = self.policy.act(state)\n",
    "\n",
    "    self.actions.append(action)\n",
    "    self.logprobs.append(logprob)\n",
    "    self.values.append(value)\n",
    "\n",
    "    return action.detach().cpu().numpy()\n",
    "    \n",
    "  def get_value(self,state):\n",
    "    with torch.no_grad():\n",
    "      state = torch.tensor(state,dtype=torch.float32).to(device)\n",
    "      value =self.policy.critic(state)\n",
    "    return value        \n",
    "\n",
    "  def update(self):\n",
    "    rewards = torch.tensor(self.rewards, dtype=torch.float32).to(device)\n",
    "    advantages = torch.tensor(self.gae_advs, dtype=torch.float32).to(device)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std()+1e-6)\n",
    "    \n",
    "    old_states = torch.stack(self.states).to(device)\n",
    "    old_actions = torch.stack(self.actions).to(device)\n",
    "    old_logprobs = torch.tensor(self.logprobs, dtype=torch.float32).to(device)\n",
    "  \n",
    " \n",
    "    for _ in range(self.n_epochs):\n",
    "\n",
    "      logprobs, new_values = self.policy.evaluate(old_states, old_actions)\n",
    "      new_values = torch.squeeze(new_values)\n",
    "      ratios = torch.exp(logprobs - old_logprobs)\n",
    "\n",
    "      surrogate = ratios * advantages\n",
    "      cliped_surrogate = torch.clamp(ratios, 1-self.clip_range, 1+self.clip_range) * advantages\n",
    "\n",
    "      loss = -torch.min(surrogate, cliped_surrogate)\n",
    "      loss+= self.vf_coef*self.value_loss(new_values, rewards)\n",
    "      loss=loss.mean()\n",
    "        \n",
    "      self.optimizer.zero_grad()\n",
    "\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(self.policy.parameters(),max_grad_norm)\n",
    "      self.optimizer.step()\n",
    "\n",
    "    self.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DQZWuSeclI-X",
   "metadata": {
    "id": "DQZWuSeclI-X"
   },
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6OSwygLg9At",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.710005Z",
     "iopub.status.busy": "2021-12-20T09:01:36.709048Z",
     "iopub.status.idle": "2021-12-20T09:01:36.713949Z",
     "shell.execute_reply": "2021-12-20T09:01:36.714521Z"
    },
    "id": "w6OSwygLg9At"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "env = gym.make(\"Walker2DCustomBulletEnv-v0\")\n",
    "\n",
    "max_episode = env._max_episode_steps       \n",
    "max_inters = int(3e6)                      # max environment interacitons\n",
    "print_freq = 10000\n",
    "print_trials = 500          \n",
    "save_freq = int(1e5)                      \n",
    "hidden_dim = 256                          \n",
    "buffer_size = 1024                        # number of interactions to buffer\n",
    "n_epochs = 20                             # number of gradient descent steps\n",
    "clip_range = 0.4                          # PPO clipped parameter\n",
    "gamma = 0.99                              # reward discount factor\n",
    "gae_lambda = .90                          # GAE factor\n",
    "max_grad_norm = 0.5                       # Gradient clipping factor\n",
    "learning_rate = 3e-5                      # learning rate\n",
    "vf_coef = 0.5                             # Value function loss coef\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFTzzKY4lPmQ",
   "metadata": {
    "id": "XFTzzKY4lPmQ"
   },
   "source": [
    "Intantiate PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GeU2gNTq8oQe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.719861Z",
     "iopub.status.busy": "2021-12-20T09:01:36.719065Z",
     "iopub.status.idle": "2021-12-20T09:01:36.726082Z",
     "shell.execute_reply": "2021-12-20T09:01:36.725481Z"
    },
    "id": "GeU2gNTq8oQe"
   },
   "outputs": [],
   "source": [
    "agent = PPO(state_dim,\n",
    "                action_dim,\n",
    "                learning_rate,\n",
    "                gamma,\n",
    "                gae_lambda,\n",
    "                n_epochs,\n",
    "                clip_range,\n",
    "                max_grad_norm,\n",
    "                hidden_dim,\n",
    "                vf_coef\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TQosNUYhliSL",
   "metadata": {
    "id": "TQosNUYhliSL"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_history=[]\n",
    "\n",
    "def train(start_iters, end_iters, start_episode):\n",
    "    i = start_iters\n",
    "    episode = start_episode\n",
    "    \n",
    "    while i < end_iters:\n",
    "      episode_reward = 0\n",
    "\n",
    "      state = env.reset()\n",
    "\n",
    "      for t in range(1, max_episode+1):\n",
    "\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.rewards.append(reward)\n",
    "\n",
    "        i +=1\n",
    "        episode_reward += reward\n",
    "\n",
    "        if i % buffer_size == 0:                        #PPO Update once buffer Full\n",
    "          value = agent.get_value(state)\n",
    "          agent.traj_update(value)\n",
    "          agent.update()\n",
    "\n",
    "        if i % save_freq == 0:\n",
    "          torch.save(agent.policy.state_dict(),'agent.pt')\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "\n",
    "          mean_reward = np.mean(rewards_history[-print_trials:])\n",
    "\n",
    "          print(f\"Interaction :{i:7}\\tEpisode :{episode:5}\\tMean Reward : {mean_reward:5.1f}\")\n",
    "\n",
    "        if done:\n",
    "          agent.traj_update(0)\n",
    "          break\n",
    "\n",
    "      episode += 1\n",
    "      rewards_history.append(episode_reward)\n",
    "\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5918af6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-20T09:01:36.758385Z",
     "iopub.status.busy": "2021-12-20T09:01:36.757452Z",
     "iopub.status.idle": "2021-12-20T11:29:59.211389Z",
     "shell.execute_reply": "2021-12-20T11:29:59.211964Z"
    },
    "id": "f5918af6",
    "outputId": "8a26dd2d-ce5e-42da-a2c5-dbb65b12e1fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(0,max_inters,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4245c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_rewards(reward_list,trials,start_epoch):\n",
    "    data_list = np.array(reward_list)\n",
    "    mean_data_list = np.convolve(data_list,np.ones(trials),'valid')/trials\n",
    "    plt.figure(figsize=(8,8))\n",
    "    x = np.arange(start_epoch, start_epoch + mean_data_list.size)\n",
    "    plt.plot(x,mean_data_list, label='Mean Train Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Mean Reward', fontsize=20)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7JWvsbIuDye",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "execution": {
     "iopub.execute_input": "2021-12-20T11:29:59.221751Z",
     "iopub.status.busy": "2021-12-20T11:29:59.219322Z",
     "iopub.status.idle": "2021-12-20T11:30:00.851074Z",
     "shell.execute_reply": "2021-12-20T11:30:00.851648Z"
    },
    "id": "A7JWvsbIuDye",
    "outputId": "d33295de-4842-4fb4-ce4d-2db0747def27",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rewards(rewards_history,print_trials,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gy93C3PVl1gt",
   "metadata": {
    "id": "gy93C3PVl1gt"
   },
   "source": [
    "Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25ojhJ3I1q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-20T11:30:00.857440Z",
     "iopub.status.busy": "2021-12-20T11:30:00.856755Z",
     "iopub.status.idle": "2021-12-20T11:30:00.868514Z",
     "shell.execute_reply": "2021-12-20T11:30:00.869096Z"
    },
    "id": "5f25ojhJ3I1q",
    "outputId": "364178c1-7cea-45d4-afc3-acd8106402e8"
   },
   "outputs": [],
   "source": [
    "del agent\n",
    "agent = PPO(state_dim,\n",
    "                action_dim,\n",
    "                learning_rate,\n",
    "                gamma,\n",
    "                gae_lambda,\n",
    "                n_epochs,\n",
    "                clip_range,\n",
    "                max_grad_norm,\n",
    "                hidden_dim,\n",
    "                vf_coef\n",
    "                )\n",
    "agent.policy.load_state_dict(torch.load('agent.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reqvq9K2l9fH",
   "metadata": {
    "id": "reqvq9K2l9fH"
   },
   "source": [
    "Rendering of Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5MB4IlNEnOWo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "execution": {
     "iopub.execute_input": "2021-12-20T11:30:00.877168Z",
     "iopub.status.busy": "2021-12-20T11:30:00.876478Z",
     "iopub.status.idle": "2021-12-20T11:35:34.669536Z",
     "shell.execute_reply": "2021-12-20T11:35:34.670115Z"
    },
    "id": "5MB4IlNEnOWo",
    "outputId": "50cbe4d1-c983-45c4-a180-500ee2535926",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import time\n",
    "\n",
    "env = gym.make('Walker2DCustomBulletEnv-v0')\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for i in range(200):\n",
    "  try:\n",
    "    rendering = env.render(mode='rgb_array')\n",
    "  except:\n",
    "    break\n",
    "  if rendering.shape != (240,320,3):\n",
    "    break\n",
    "  img.set_data(rendering)\n",
    "  display.display(plt.gcf())    \n",
    "  display.clear_output(wait=True)\n",
    "  for _ in range(20):\n",
    "    action = agent.act(state)\n",
    "    state , r, done, _ = env.step(action)\n",
    "    if done: break\n",
    "    time.sleep(1/20.)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d5ae2",
   "metadata": {},
   "source": [
    "#References\n",
    "\n",
    "Barhate, N.. (2021). Minimal PyTorch Implementation of Proximal Policy Optimization. https://github.com/nikhilbarhate99/PPO-PyTorch.\n",
    "\n",
    "Achiam, J. 2018. Spinning Up in Deep Reinforcement Learning. \n",
    "\n",
    "Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann 2021. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Research, 22(268), p.1-8.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_dk1384.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
